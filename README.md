# Nano Performance Test

## Basic information
- Official website: https://www.aicrowd.com/challenges/esci-challenge-for-improving-product-search
- Baseline model: https://github.com/amazon-research/esci-code
- Data located at:
    - root@Almaren-Node-105:/disk0/amazonKDD2022
    - hdfs://172.16.0.105:8020/amazonKDD2022

## Prepare the environment
```bash
conda create -n recsys python=3.7
conda activate recsys
conda install pytorch==1.11.0 torchvision==0.12.0 cpuonly -c pytorch
pip install pytorch-lightning==1.6.4
pip install transformers[torch]==4.11.0
pip install --pre --upgrade bigdl-nano[pytorch]
source bigdl-nano-init
```

## Start training
```bash
bash launch_training.sh
```

__Options:__
* `model_type`: **Required.** The model used for training, either "CrossEncoder" or "GCN".
* `train_path`: **Required.** The path to input training CSV with pairs of queries and products.
* `product_path`: **Required.** The path to input product catalogue CSV.
* `nano`: Whether use `nano.pytorch.Trainer` to train the model. Default to be False.
* `use_ipex`: Options for `nano.pytorch.Trainer`, only used when `nano` is True. Default to be False.
* `enable_bf16`: Options for `nano.pytorch.Trainer`, only used when `nano` is True. Default to be False.
* `channels_last`: Options for `nano.pytorch.Trainer`, only used when `nano` is True. Default to be False.
* `num_processes`: The number of processes in distributed training, only used when `nano` is True. Default to be 1.
* `num_epochs`: The number of epochs for training. Default to be 1.
* `dev_ratio`: The ratio of datasets sampled for development. Set to None to use full datasets. Default to be None.
* `test_set_size`: The ratio of train to test subsets split randomly. Default to be 0.1.
* `val_check_interval`: The frequency to check the validation set within one epoch. Default to be 1.
* `random_state`: The random seed which is set to make results reproducible. Default to be 42.
* `train_batch_size`: The batch size for training. Default to be 32.
* `eval_batch_size`: The batch size for evaluation. Default to be 32.
* `num_neighbors`: The number of neighbor queries for each product, only used when model_type is GCN. Default to be 1.

## Dataset introduction (Shopping Queries Dataset)
The Shopping Queries Dataset is a large-scale manually annotated data set composed of challenging customer queries.

The dataset contains a list of query-result pairs with annotated `E/S/C/I` labels. The data is multilingual and it includes queries from English, Japanese, and Spanish languages. The examples in the dataset have the following fields: `example_id`, `query`, `query_id`, `product`, `product_title`, `product_description`, `product_bullet_point`, `product_brand`, `product_color`, `product_locale`, and `esci_label`.

The training set contains 33,804 unique queries and 781,744 rows corresponding each to a `<query, item>` judgement. A summary of the Shopping Queries Dataset is given in the table below. The table includes the number of unique queries, the number of judgements, and the average number of judgements per query (i.e., average depth) across the three different languages.

| Language | #Queries | #Judgements | Avg. Depth |
| :--- | :---: | :---: | :---: |
| English | 20,888 | 419,730 | 20.1 |
| Spanish | 5,632 | 152,920 | 27.2 |
| Japanese | 7,284 | 209,094 | 28.7 |
| US+ES+JP | 33,804 | 781,744 | 23.1 |

## Task description (Amazon KDD Cup '22 Task 1)
Given a user specified query and a list of matched products, the goal of this task is to rank the products so that the relevant products are ranked above the non-relevant ones. This is similar to standard information retrieval tasks, but specifically in the context of product search in e-commerce. The input for this task will be a list of queries with their identifiers. The system will have to output a list where for each `query_id` the first row will be the most relevant product and the last row the least relevant product.

## Model architecture
### 1. Cross Encoder[<sup>[1]</sup>](#refer-anchor)
For a Cross Encoder, we pass both queries and product text simultaneously to the transformer network. The text of products is generated by concatenating product contents (title, description, brand, etc.) as follows.
```
[CLS] <cleaned_title_content> [SEP] <clenaed_description_content> [SEP] ...
```
where `[CLS]` and `[SEP]` are the special tokens.

It then produces the hidden representations for each query-product pair of size `(seq_len, hidden_size)` from which we can either directly get the CLS token or take the average of last four layers of hidden states. The CLS token will be passed to a fully connected classifier to output the probabilities of four labels `E/S/C/I` which can be used to calculate the expected gain below for ranking.
$$P_E \times 1 + P_S \times 0.1 + P_C \times 0.01$$ 

To be noted, there are differences between directly getting the CLS token from pooler outputs and the average of hidden layers, which are demonstrated in [this issue](https://github.com/huggingface/transformers/issues/7540).

### 2. GCN[<sup>[2]</sup>](#refer-anchor)
The model has two main components: 
1. A **query encoder** that encodes search queries.
2. A **product encoder** that encodes both the product description and its neighbor queries. The product encoder has a GCN component that encapsulates the neighbor queries and product information.

The embeddings outputted from encoders above can then be compared using cosine similarity which will produce a value between 0 and 1 indicating the expected score of the query-product pair.

#### Query encoder
A multilingual BERT is used to encode queries. However, the encoder could be any transformer-based encoder. We use the last hidden states of the encoders’ CLS token as the embeddings for the query.

#### Product encoder
The product encoder consists of:
1. A multilingual BERT to extract the features of a product and its neighbor queries.
2. A graph convolution layer that aggregates the extracted features to compute the final embeddings for a given product.

The transformer-based encoder layer shares its parameters with the query encoder, and we also use the last hidden states of the encoder’s CLS token as the features.

The operations in the graph convolution layer are described below,

$$
\begin{align}
\mathbf{h_\mathit{q_i}} &= \frac{1}{t}\sum_j \text{ReLU}(\mathbf{W_q} \cdot \mathbf{h_\mathit{q_{i\ ,\ j}}} + \mathbf{b_q})\\
\mathbf{x_\mathit{p_i}} &= \text{ReLU}(\mathbf{W_p} \cdot \text{CONCAT}(\mathbf{h_\mathit{p_i}}\ ,\mathbf{h_\mathit{q_i}}) + \mathbf{b_p})
\end{align}
$$

where $\mathbf{h_\mathit{p_i}}$ is the extracted feature of product $p_i$, and $\mathbf{h_\mathit{q_{i\ ,1}}}\ , \dots, \mathbf{h_\mathit{q_{i\ ,\ t}}}$ is the extracted feature of $p_i$'s neighbor queries $q_\mathit{i,1}, \dots, q_{i,t}$.

## Results
### 1. Cross Encoder

#### Hyperparam settings
| Batch Size | Dev Ratio | Test Set Size | #Epochs | Max LR | Weight Decay |
| :---: | :---: | :---: | :---: | :---: | :---: |
| 32 | 0.05 | 0.05 | 2 | 1e-4 | 1e-2 |

#### Test results
| #Procs | `ipex` | `bf16` | `ch_last` | Fit Time (s) | Loss (CE) | Accuracy (%) |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Raw | N/A | N/A | N/A | 6166.67 | 1.2055 | 50.8709 |
| 1 | False | False | False | 5933.29 | 1.2055 | 50.8709 |
| **4** | False | False | False | 4799.86 | **1.1799** | **55.5208** |
| 4 | **True** | False | False | 4776.84 | 1.1866 | 53.8541 |
| 4 | False | **True** | False | 4779.56 | 1.1799 | 55.5208 |
| 4 | False | False | **True** | 4802.48 | 1.1799 | 55.5208 |
| **8** | False | False | False | **4488.69** | 1.1880 | 55.1897 |

The table above shows that using Nano with 8 processes is 1.37x faster than raw trainer, and the accuracy will drop by 1.67% if ipex is enabled. The accuracy drop of the raw trainer is likely due to overfitting which is caused by 4x training steps and the same learning rate as others.

### 2. GCN

#### Hyperparam settings
| Batch Size | Dev Ratio | Test Set Size | #Epochs | #Neighbors | Max LR | Weight Decay |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| 32 | 0.01 | 0.05 | 2 | 2 | 1e-4 | 1e-2 |

#### Test results
| #Procs | `ipex` | `bf16` | `ch_last` | Fit Time (s) | Loss (MSE) | nDCG@10 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| Raw | N/A | N/A | N/A | 6524.73 | **0.0663** | **0.8770** |
| 1 | False | False | False | 6601.22 | 0.0663 | 0.8770 |
| **2** | False | False | False | 5410.66 | 0.0690 | 0.8747 |
| **4** | False | False | False | 5111.91 | 0.0685 | 0.8768 |
| 4 | **True** | False | False | **4910.27** | 0.0689 | 0.8759 |
| 4 | False | **True** | False | 4974.92 | 0.0685 | 0.8768 |
| 4 | False | False | **True** | 5052.66 | 0.0685 | 0.8768 |

Due to the memory limitation, we can use 4 processes in parallel for GCN at most. The table above shows that using Nano with 4 processes and ipex is 1.33x faster than raw trainer, and the nDCG will drop by 0.09% if ipex is enabled.


<div id="refer-anchor"></div>

## Reference
[1] [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/pdf/1908.10084.pdf)

[2] [Graph-based Multilingual Product Retrieval in E-Commerce Search](https://arxiv.org/pdf/2105.02978.pdf)
